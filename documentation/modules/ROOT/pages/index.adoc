= {product-long-connectors} Workshop
:page-layout: home
:!sectids:
include::_attributes.adoc[]


[.text-center.strong]
== Introduction

Welcome to the {product-long-connectors} workshop.

As a developer of applications and services, you can use {product-long-connectors} to create and configure connections between {product-long-kafka} and third-party systems.

{product-long-kafka} is a cloud service that simplifies the process of running Apache Kafka. Apache Kafka is an open-source, distributed, publish-subscribe messaging system for creating fault-tolerant, real-time data feeds. {product-long-kafka} enables you to add Kafka data-streaming functionality in your applications without having to install, configure, run, and maintain your own Kafka clusters.

You can use {product-connectors} to configure communication between Streams for Apache Kafka instances and external services and applications. {product-connectors} allow you to configure how data moves from one endpoint to another without writing code.

The following diagram illustrates how data flows from a data source through a data source connector to a Kafka topic. And how data flows from a Kafka topic to a data sink through a data sink connector.

.{product-connectors} data flow
image::connectors-diagram.png[]

At the moment there are more than 50 source and sink connectors available. These include source and sink connectors to a variety of cloud services based on the awesome https://developers.redhat.com/topics/camel-k[Camel-K] technology, as well as source connectors for databases based on the popular https://debezium.io[Debezium] project for Change Data Capture. The complete list of available connectors to date can be found https://www.redhat.com/en/technologies/cloud-computing/openshift/connectors[here]. 

In this workshop you will:

* Provision a Kafka instance using {product-long-kafka}.
* Configure the {product-kafka} instance for use with {product-long-connectors}.
* Create a {product-short-connectors} instance as a data source.
* Create a {product-short-connectors} instance as a data sink.
* Create a {product-short-connectors} instance to capture data change events from a cloud database.

[.tiles.browse]
== Browse modules

[.tile]
.xref:01-provision-kafka-instance.adoc[1. Provision a Kafka instance in {product-long-kafka}]
* xref:01-provision-kafka-instance.adoc#redhat_account[Create a Red Hat Account]
* xref:01-provision-kafka-instance.adoc#kafka[Provision a Kafka instance]
* xref:01-provision-kafka-instance.adoc#configure_kafka[Configure the instance of {product-kafka} for use with {product-connectors}]

.xref:02-source-connector.adoc[2. Create source and sink {product-connectors} instances]
* xref:02-source-connector.adoc#create_source_connector[Create a {product-short-connectors} instance for a data source]
* xref:02-source-connector.adoc#create_sink_connector[Create a {product-short-connectors} instance for a data sink]

.xref:03-source-cdc-connector.adoc[3. Create a source {product-connectors} instance for Change Data Capture]
* xref:03-source-cdc-connector.adoc#provision_mongo[Provision a MongoDB Atlas instance]
* xref:03-source-cdc-connector.adoc#create_cdc_connector[Create a {product-short-connectors} instance for Change Data Capture with MongoDB]
* xref:03-source-cdc-connector.adoc#capture_change_events[Capture Change Data Events from MongoDB]
